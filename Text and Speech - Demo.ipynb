{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to AI\n",
    "## Text and Speech Demos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Frequency Analysis\n",
    "\n",
    "#### Load a Text Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We set sail on this new sea because there is new knowledge to be gained, and new rights to be won, and they must be won and used for the progress of all people. For space science, like nuclear science and all technology, has no conscience of its own. Whether it will become a force for good or ill depends on man, and only if the United States occupies a position of pre-eminence can we help decide whether this new ocean will be a sea of peace or a new terrifying theater of war. I do not say that we should or will go unprotected against the hostile misuse of space any more than we go unprotected against the hostile use of land or sea, but I do say that space can be explored and mastered without feeding the fires of war, without repeating the mistakes that man has made in extending his writ around this globe of ours.\n",
      "There is no strife, no prejudice, no national conflict in outer space as yet. Its hazards are hostile to us all. Its conquest deserves the best of all mankind, and its opportunity for peaceful cooperation may never come again. But why, some say, the Moon? Why choose this as our goal? And they may well ask, why climb the highest mountain? Why, 35 years ago, fly the Atlantic? Why does Rice play Texas?\n",
      "We choose to go to the Moon! We choose to go to the Moon in this decade and do the other things, not because they are easy, but because they are hard; because that goal will serve to organize and measure the best of our energies and skills, because that challenge is one that we are willing to accept, one we are unwilling to postpone, and one we intend to win!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100  1589  100  1589    0     0   2019      0 --:--:-- --:--:-- --:--:--  2016\n"
     ]
    }
   ],
   "source": [
    "# Use Curl to get a document from GitHub and open it\n",
    "!curl https://raw.githubusercontent.com/MicrosoftLearning/AI-Introduction/master/files/Moon.txt -o Moon.txt\n",
    "doc1 = open(\"Moon.txt\", \"r\")\n",
    "\n",
    "# Read the document and print its contents\n",
    "doc1Txt = doc1.read()\n",
    "print(doc1Txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize the Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "\n",
    "# remove numeric digits\n",
    "txt = ''.join(c for c in doc1Txt if not c.isdigit())\n",
    "\n",
    "# remove punctuation and make lower case\n",
    "txt = ''.join(c for c in txt if c not in punctuation).lower()\n",
    "\n",
    "# print the normalized text\n",
    "print (txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the Frequency Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.probability import FreqDist\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "# Tokenize the text into individual words\n",
    "words = nltk.tokenize.word_tokenize(txt)\n",
    "\n",
    "# Get the frequency distribution of the words into a data frame\n",
    "fdist = FreqDist(words)\n",
    "count_frame = pd.DataFrame(fdist, index =[0]).T\n",
    "count_frame.columns = ['Count']\n",
    "print (count_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the distribution as a pareto chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sort the data frame by frequency\n",
    "counts = count_frame.sort_values('Count', ascending = False)\n",
    "\n",
    "# Display the top 60 words as a bar plot\n",
    "fig = plt.figure(figsize=(16, 9))\n",
    "ax = fig.gca()    \n",
    "counts['Count'][:60].plot(kind = 'bar', ax = ax)\n",
    "ax.set_title('Frequency of the most common words')\n",
    "ax.set_ylabel('Frequency of word')\n",
    "ax.set_xlabel('Word')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get standard stop words from NLTK\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Filter out the stop words\n",
    "txt = ' '.join([word for word in txt.split() if word not in (stopwords.words('english'))])\n",
    "\n",
    "# Get the frequency distribution of the remaining words\n",
    "words = nltk.tokenize.word_tokenize(txt)\n",
    "fdist = FreqDist(words)\n",
    "count_frame = pd.DataFrame(fdist, index =[0]).T\n",
    "count_frame.columns = ['Count']\n",
    "\n",
    "# Plot the frequency of the top 60 words\n",
    "counts = count_frame.sort_values('Count', ascending = False)\n",
    "fig = plt.figure(figsize=(16, 9))\n",
    "ax = fig.gca()    \n",
    "counts['Count'][:60].plot(kind = 'bar', ax = ax)\n",
    "ax.set_title('Frequency of the most common words')\n",
    "ax.set_ylabel('Frequency of word')\n",
    "ax.set_xlabel('Word')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term Frequency - Inverse Document Frequency\n",
    "#### View the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# remind ourselves of the first document\n",
    "print(doc1Txt)\n",
    "print(\"------------------------------------------------\")\n",
    "\n",
    "# Get a second document, normalize it, and remove stop words\n",
    "!curl https://raw.githubusercontent.com/MicrosoftLearning/AI-Introduction/master/files/Gettysburg.txt -o Gettysburg.txt\n",
    "doc2 = open(\"Gettysburg.txt\", \"r\")\n",
    "doc2Txt = doc2.read()\n",
    "print (doc2Txt)\n",
    "from string import punctuation\n",
    "txt2 = ''.join(c for c in doc2Txt if not c.isdigit())\n",
    "txt2 = ''.join(c for c in txt2 if c not in punctuation).lower()\n",
    "txt2 = ' '.join([word for word in txt2.split() if word not in (stopwords.words('english'))])\n",
    "\n",
    "\n",
    "# and a third\n",
    "print(\"------------------------------------------------\")\n",
    "!curl https://raw.githubusercontent.com/MicrosoftLearning/AI-Introduction/master/files/Cognitive.txt -o Cognitive.txt\n",
    "doc3 = open(\"Cognitive.txt\", \"r\")\n",
    "doc3Txt = doc3.read()\n",
    "print (doc3Txt)\n",
    "from string import punctuation\n",
    "txt3 = ''.join(c for c in doc3Txt if not c.isdigit())\n",
    "txt3 = ''.join(c for c in txt3 if c not in punctuation).lower()\n",
    "txt3 = ' '.join([word for word in txt3.split() if word not in (stopwords.words('english'))])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get TF-IDF Values for the top three words in each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# install textblob library and define functions for TF-IDF\n",
    "!pip install -U textblob\n",
    "import math\n",
    "from textblob import TextBlob as tb\n",
    "\n",
    "def tf(word, doc):\n",
    "    return doc.words.count(word) / len(doc.words)\n",
    "\n",
    "def contains(word, docs):\n",
    "    return sum(1 for doc in docs if word in doc.words)\n",
    "\n",
    "def idf(word, docs):\n",
    "    return math.log(len(docs) / (1 + contains(word, docs)))\n",
    "\n",
    "def tfidf(word, doc, docs):\n",
    "    return tf(word,doc) * idf(word, docs)\n",
    "\n",
    "\n",
    "# Create a collection of documents as textblobs\n",
    "doc1 = tb(txt)\n",
    "doc2 = tb(txt2)\n",
    "doc3 = tb(txt3)\n",
    "docs = [doc1, doc2, doc3]\n",
    "\n",
    "# Use TF-IDF to get the three most important words from each document\n",
    "print('-----------------------------------------------------------')\n",
    "for i, doc in enumerate(docs):\n",
    "    print(\"Top words in document {}\".format(i + 1))\n",
    "    scores = {word: tfidf(word, doc, docs) for word in doc.words}\n",
    "    sorted_words = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    for word, score in sorted_words[:3]:\n",
    "        print(\"\\tWord: {}, TF-IDF: {}\".format(word, round(score, 5)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "\n",
    "#### View frequency of unstemmed words from Kennedy's inauguration speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load and print text\n",
    "!curl https://raw.githubusercontent.com/MicrosoftLearning/AI-Introduction/master/files/KennedyInaugural.txt -o KennedyInaugural.txt\n",
    "doc4 = open(\"KennedyInaugural.txt\", \"r\")\n",
    "kenTxt = doc4.read()\n",
    "\n",
    "print(kenTxt)\n",
    "\n",
    "# Normalize and remove stop words\n",
    "from string import punctuation\n",
    "kenTxt = ''.join(c for c in kenTxt if not c.isdigit())\n",
    "kenTxt = ''.join(c for c in kenTxt if c not in punctuation).lower()\n",
    "kenTxt = ' '.join([word for word in kenTxt.split() if word not in (stopwords.words('english'))])\n",
    "\n",
    "# Get Frequency distribution\n",
    "words = nltk.tokenize.word_tokenize(kenTxt)\n",
    "fdist = FreqDist(words)\n",
    "count_frame = pd.DataFrame(fdist, index =[0]).T\n",
    "count_frame.columns = ['Count']\n",
    "\n",
    "# Plot frequency\n",
    "counts = count_frame.sort_values('Count', ascending = False)\n",
    "fig = plt.figure(figsize=(16, 9))\n",
    "ax = fig.gca()    \n",
    "counts['Count'][:60].plot(kind = 'bar', ax = ax)\n",
    "ax.set_title('Frequency of the most common words')\n",
    "ax.set_ylabel('Frequency of word')\n",
    "ax.set_xlabel('Word')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stem the words using the Porter stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# Get the word stems\n",
    "ps = PorterStemmer()\n",
    "stems = [ps.stem(word) for word in words]\n",
    "\n",
    "# Get Frequency distribution\n",
    "fdist = FreqDist(stems)\n",
    "count_frame = pd.DataFrame(fdist, index =[0]).T\n",
    "count_frame.columns = ['Count']\n",
    "\n",
    "# Plot frequency\n",
    "counts = count_frame.sort_values('Count', ascending = False)\n",
    "fig = plt.figure(figsize=(16, 9))\n",
    "ax = fig.gca()    \n",
    "counts['Count'][:60].plot(kind = 'bar', ax = ax)\n",
    "ax.set_title('Frequency of the most common words')\n",
    "ax.set_ylabel('Frequency of word')\n",
    "ax.set_xlabel('Word')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linguistic Analytics\n",
    "\n",
    "https://azure.microsoft.com/en-us/services/cognitive-services/linguistic-analysis-api/\n",
    "\n",
    "#### Use the Linguistics Analytics API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import http.client, urllib.request, urllib.parse, urllib.error, base64, json\n",
    "\n",
    "myText = input('Please enter some text: \\n')\n",
    "\n",
    "headers = {\n",
    "    # Request headers\n",
    "    'Content-Type': 'application/json',\n",
    "    'Ocp-Apim-Subscription-Key': 'YOUR_KEY_HERE',\n",
    "}\n",
    "\n",
    "params = urllib.parse.urlencode({\n",
    "})\n",
    "\n",
    "body = {\n",
    "    \"language\" : \"en\",\n",
    "    \"analyzerIds\" : [\"4fa79af1-f22c-408d-98bb-b7d7aeef7f04\", \"22a6b758-420f-4745-8a3c-46835a67c0d2\"],\n",
    "    \"text\" : myText\n",
    "}\n",
    "\n",
    "try:\n",
    "    conn = http.client.HTTPSConnection('westus.api.cognitive.microsoft.com')\n",
    "    conn.request(\"POST\", \"/linguistics/v1.0/analyze?%s\" % params, str(body), headers)\n",
    "    response = conn.getresponse()\n",
    "    data = response.read()\n",
    "    parsed = json.loads(data)\n",
    "    for analyzer in parsed:\n",
    "        print(\"Analyzer: \" + analyzer[\"analyzerId\"])\n",
    "        print(analyzer[\"result\"])\n",
    "        print(\"---------------------------\")\n",
    "    conn.close()\n",
    "except Exception as e:\n",
    "    print(\"[Errno {0}] {1}\".format(e.errno, e.strerror))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Analytics\n",
    "#### Create a Text Analytics service in Azure\n",
    "https://portal.azure.com\n",
    "\n",
    "#### Get the region-specific URI and Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "textAnalyticsURI = 'REGION.api.cognitive.microsoft.com'\n",
    "textKey = 'YOUR_KEY_HERE'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyze the Gettysburg and Cognitive documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import http.client, urllib.request, urllib.parse, urllib.error, base64, json, urllib\n",
    "\n",
    "# Define the request headers.\n",
    "headers = {\n",
    "    'Content-Type': 'application/json',\n",
    "    'Ocp-Apim-Subscription-Key': textKey,\n",
    "    'Accept': 'application/json'\n",
    "}\n",
    "\n",
    "# Define the parameters\n",
    "params = urllib.parse.urlencode({\n",
    "})\n",
    "\n",
    "# Define the request body\n",
    "body = {\n",
    "  \"documents\": [\n",
    "    {\n",
    "        \"language\": \"en\",\n",
    "        \"id\": \"1\",\n",
    "        \"text\": doc2Txt\n",
    "    },\n",
    "    {\n",
    "        \"language\": \"en\",\n",
    "        \"id\": \"2\",\n",
    "        \"text\": doc3Txt\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "try:\n",
    "    # Execute the REST API call and get the response.\n",
    "    conn = http.client.HTTPSConnection(textAnalyticsURI)\n",
    "    conn.request(\"POST\", \"/text/analytics/v2.0/keyPhrases?%s\" % params, str(body), headers)\n",
    "    response = conn.getresponse()\n",
    "    data = response.read().decode(\"UTF-8\")\n",
    "\n",
    "    # 'data' contains the JSON response, which includes a collection of documents.\n",
    "    parsed = json.loads(data)\n",
    "    for document in parsed['documents']:\n",
    "        print(\"Document \" + document[\"id\"] + \" key phrases:\")\n",
    "        for phrase in document['keyPhrases']:\n",
    "            print(\"  \" + phrase)\n",
    "        print(\"---------------------------\")\n",
    "    conn.close()\n",
    "\n",
    "except Exception as e:\n",
    "    print('Error:')\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "body = {\n",
    "  \"documents\": [\n",
    "    {\n",
    "      \"language\": \"en\",\n",
    "      \"id\": \"1\",\n",
    "      \"text\": \"Wow! cognitive services are fantastic.\"\n",
    "    },\n",
    "    {\n",
    "      \"language\": \"en\",\n",
    "      \"id\": \"2\",\n",
    "      \"text\": \"I hate it when computers don't understand me.\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "\n",
    "try:\n",
    "    conn = http.client.HTTPSConnection(textAnalyticsURI)\n",
    "    conn.request(\"POST\", \"/text/analytics/v2.0/sentiment?%s\" % params, str(body), headers)\n",
    "    response = conn.getresponse()\n",
    "    data = response.read().decode(\"UTF-8\")\n",
    "    parsed = json.loads(data)\n",
    "    \n",
    "    # Get the numeric score for each document\n",
    "    for document in parsed['documents']:\n",
    "        sentiment = \"negative\"\n",
    "        \n",
    "        # if it's more than 0.5, consider the sentiment to be positive.\n",
    "        if document[\"score\"] >= 0.5:\n",
    "            sentiment = \"positive\"\n",
    "        print(\"Document:\" + document[\"id\"] + \" = \" + sentiment)\n",
    "    conn.close()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"[Errno {0}] {1}\".format(e.errno, e.strerror))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speech\n",
    "#### Create a Bing Speech API service\n",
    "https://portal.azure.com\n",
    "#### Get the service key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "speechKey = '4a12b8642689440eb779ffa7a84600aa'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install SpeechRecognition package\n",
    "https://pypi.python.org/pypi/SpeechRecognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install SpeechRecognition\n",
    "!pip install pyaudio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert speech to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Say something!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-4ac71f93e791>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0msr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMicrophone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msource\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Say something!\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0maudio\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlisten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# transcribe speech using the Bing Speech API\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Microsoft SQL Server\\MSSQL14.SQLEXPRESS\\PYTHON_SERVICES\\lib\\site-packages\\speech_recognition\\__init__.py\u001b[0m in \u001b[0;36mlisten\u001b[1;34m(self, source, timeout, phrase_time_limit, snowboy_configuration)\u001b[0m\n\u001b[0;32m    650\u001b[0m                     \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    651\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 652\u001b[1;33m                 \u001b[0mbuffer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msource\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCHUNK\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    653\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mbreak\u001b[0m  \u001b[1;31m# reached end of the stream\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    654\u001b[0m                 \u001b[0mframes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Microsoft SQL Server\\MSSQL14.SQLEXPRESS\\PYTHON_SERVICES\\lib\\site-packages\\speech_recognition\\__init__.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 161\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyaudio_stream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexception_on_overflow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Microsoft SQL Server\\MSSQL14.SQLEXPRESS\\PYTHON_SERVICES\\lib\\site-packages\\pyaudio.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, num_frames, exception_on_overflow)\u001b[0m\n\u001b[0;32m    606\u001b[0m                           paCanNotReadFromAnOutputOnlyStream)\n\u001b[0;32m    607\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 608\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mpa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_stream\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stream\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_frames\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexception_on_overflow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    609\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    610\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_read_available\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import speech_recognition as sr\n",
    "\n",
    "# Read the audio file\n",
    "r = sr.Recognizer()\n",
    "with sr.Microphone() as source:\n",
    "    print(\"Say something!\")\n",
    "    audio = r.listen(source)\n",
    "\n",
    "# transcribe speech using the Bing Speech API\n",
    "try:\n",
    "    transcription = r.recognize_bing(audio, key=speechKey)\n",
    "    print(\"Here's what I heard:\")\n",
    "    print('\"' + transcription + '\"')\n",
    "    \n",
    "except sr.UnknownValueError:\n",
    "    print(\"The audio was unclear\")\n",
    "except sr.RequestError as e:\n",
    "    print (e)\n",
    "    print(\"Something went wrong :-(; {0}\".format(e))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert text to speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import IPython\n",
    "import http.client, urllib.parse, json\n",
    "from xml.etree import ElementTree\n",
    "\n",
    "# Get the input text\n",
    "myText = input('What would you like me to say?: \\n')\n",
    "\n",
    "# The Speech API requires an access token (valid for 10 mins)\n",
    "apiKey = speechKey\n",
    "params = \"\"\n",
    "headers = {\"Ocp-Apim-Subscription-Key\": apiKey}\n",
    "AccessTokenHost = \"api.cognitive.microsoft.com\"\n",
    "path = \"/sts/v1.0/issueToken\"\n",
    "\n",
    "# Use the API key to request an access token\n",
    "conn = http.client.HTTPSConnection(AccessTokenHost)\n",
    "conn.request(\"POST\", path, params, headers)\n",
    "response = conn.getresponse()\n",
    "data = response.read()\n",
    "conn.close()\n",
    "accesstoken = data.decode(\"UTF-8\")\n",
    "\n",
    "# Now that we have a token, we can set up the request\n",
    "body = ElementTree.Element('speak', version='1.0')\n",
    "body.set('{http://www.w3.org/XML/1998/namespace}lang', 'en-us')\n",
    "voice = ElementTree.SubElement(body, 'voice')\n",
    "voice.set('{http://www.w3.org/XML/1998/namespace}lang', 'en-US')\n",
    "voice.set('{http://www.w3.org/XML/1998/namespace}gender', 'Male')\n",
    "voice.set('name', 'Microsoft Server Speech Text to Speech Voice (en-US, JessaRUS)')\n",
    "voice.text = myText\n",
    "headers = {\"Content-type\": \"application/ssml+xml\", \n",
    "           \"X-Microsoft-OutputFormat\": \"riff-16khz-16bit-mono-pcm\", \n",
    "           \"Authorization\": \"Bearer \" + accesstoken, \n",
    "           \"X-Search-AppId\": \"07D3234E49CE426DAA29772419F436CA\", \n",
    "           \"X-Search-ClientID\": \"1ECFAE91408841A480F00935DC390960\", \n",
    "           \"User-Agent\": \"TTSForPython\"}\n",
    "\n",
    "#Connect to server to synthesize a wav from the text\n",
    "conn = http.client.HTTPSConnection(\"speech.platform.bing.com\")\n",
    "conn.request(\"POST\", \"/synthesize\", ElementTree.tostring(body), headers)\n",
    "response = conn.getresponse()\n",
    "data = response.read()\n",
    "conn.close()\n",
    "\n",
    "#Play the wav\n",
    "IPython.display.Audio(data, autoplay=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Translation\n",
    "\n",
    "#### Create a Microsoft Text Translation Service\n",
    "https://portal.azure.com\n",
    "\n",
    "#### Get the service key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transTextKey = \"YOUR_KEY_HERE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Translate Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests, http.client, urllib.request, urllib.parse, urllib.error, base64, json, urllib\n",
    "from xml.etree import ElementTree\n",
    "\n",
    "\n",
    "textToTranslate = input('Please enter some text: \\n')\n",
    "fromLangCode = input('What language is this?: \\n') \n",
    "toLangCode = input('To what language would you like it translated?: \\n') \n",
    "\n",
    "try:\n",
    "    # Connect to server to get the Access Token\n",
    "    apiKey = transTextKey\n",
    "    params = \"\"\n",
    "    headers = {\"Ocp-Apim-Subscription-Key\": apiKey}\n",
    "    AccessTokenHost = \"api.cognitive.microsoft.com\"\n",
    "    path = \"/sts/v1.0/issueToken\"\n",
    "\n",
    "    conn = http.client.HTTPSConnection(AccessTokenHost)\n",
    "    conn.request(\"POST\", path, params, headers)\n",
    "    response = conn.getresponse()\n",
    "    data = response.read()\n",
    "    conn.close()\n",
    "    accesstoken = \"Bearer \" + data.decode(\"UTF-8\")\n",
    "\n",
    "\n",
    "    # Define the request headers.\n",
    "    headers = {\n",
    "        'Authorization': accesstoken\n",
    "    }\n",
    "\n",
    "    # Define the parameters\n",
    "    params = urllib.parse.urlencode({\n",
    "        \"text\": textToTranslate,\n",
    "        \"to\": toLangCode,\n",
    "        \"from\": fromLangCode\n",
    "    })\n",
    "\n",
    "    # Execute the REST API call and get the response.\n",
    "    conn = http.client.HTTPSConnection(\"api.microsofttranslator.com\")\n",
    "    conn.request(\"GET\", \"/V2/Http.svc/Translate?%s\" % params, \"{body}\", headers)\n",
    "    response = conn.getresponse()\n",
    "    data = response.read()\n",
    "    translation = ElementTree.fromstring(data.decode(\"utf-8\"))\n",
    "    print (translation.text)\n",
    "\n",
    "    conn.close()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"[Errno {0}] {1}\".format(e.errno, e.strerror))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Understanding Intelligence Service (LUIS)\n",
    "\n",
    "#### Provision LUIS\n",
    "https://portal.azure.com\n",
    "\n",
    "#### Create a LUIS App\n",
    "https://www.luis.ai/\n",
    "\n",
    "Home automation app with a *Light* entity and the following intents:\n",
    "- Light On\n",
    "- Light Off\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib.pyplot import imshow\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import json \n",
    "\n",
    "# Set up API configuration\n",
    "endpointUrl = \"https://eastus.api.cognitive.microsoft.com/luis/v2.0/apps/7306b6e8-0656-41c1-8dd9-2af977be639d?subscription-key=f181418faa3b4ec6aa753c518d9699a4&verbose=true&timezoneOffset=0&q=\"\n",
    "\n",
    "# prompt for a command\n",
    "command = input('Please enter a command: \\n')\n",
    "\n",
    "# Call the LUIS service and get the JSON response\n",
    "endpoint = endpointUrl + command.replace(\" \",\"+\")\n",
    "response = requests.get(endpoint)\n",
    "data = json.loads(response.content.decode(\"UTF-8\"))\n",
    "\n",
    "# Identify the top scoring intent\n",
    "intent = data[\"topScoringIntent\"][\"intent\"]\n",
    "if (intent == \"Light On\"):\n",
    "    img_url = 'https://raw.githubusercontent.com/MicrosoftLearning/AI-Introduction/master/files/LightOn.jpg'\n",
    "elif (intent == \"Light Off\"):\n",
    "    img_url = 'https://raw.githubusercontent.com/MicrosoftLearning/AI-Introduction/master/files/LightOff.jpg'\n",
    "else:\n",
    "    img_url = 'https://raw.githubusercontent.com/MicrosoftLearning/AI-Introduction/master/files/Dunno.jpg'\n",
    "\n",
    "# Get the appropriate image and show it\n",
    "response = requests.get(img_url)\n",
    "img = Image.open(BytesIO(response.content))\n",
    "imshow(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
